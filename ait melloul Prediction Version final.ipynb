{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c542496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "#import missingno as msno\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Extra Libs\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "from bokeh.models import HoverTool\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Default visual settings\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Ignore warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721dd195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading xlsx file\n",
    "df_RH=pd.read_excel(\"C:/Users/abdbe/Documents/Article ait melloul/RH101W.xlsx\")\n",
    "df_ZI=pd.read_excel(\"C:/Users/abdbe/Documents/Article ait melloul/ZI101W.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ZI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0714ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RH = df_RH.set_index(pd.DatetimeIndex(df_RH['Time']))\n",
    "\n",
    "df_ZI = df_ZI.set_index(pd.DatetimeIndex(df_ZI['Time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RH['Year'] = df_RH['Time'].apply(lambda time: time.year)\n",
    "df_RH['Month'] = df_RH['Time'].apply(lambda time: time.month)\n",
    "df_RH['Day'] = df_RH['Time'].apply(lambda t: t.day)\n",
    "df_RH['Hour'] = df_RH['Time'].apply(lambda time: time.hour)\n",
    "df_RH['Day of Week'] = df_RH['Time'].apply(lambda time: time.dayofweek)\n",
    "\n",
    "df_ZI['Day of Week'] = df_ZI['Time'].apply(lambda time: time.dayofweek)\n",
    "df_ZI['Year'] = df_ZI['Time'].apply(lambda time: time.year)\n",
    "df_ZI['Month'] = df_ZI['Time'].apply(lambda time: time.month)\n",
    "df_ZI['Day'] = df_ZI['Time'].apply(lambda t: t.day)\n",
    "df_ZI['Hour'] = df_ZI['Time'].apply(lambda time: time.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RH = df_RH.drop(['Time'], axis=1)\n",
    "\n",
    "df_ZI = df_ZI.drop(['Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RH=df_RH.resample('H').mean()\n",
    "df_ZI=df_ZI.resample('H').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ZI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_tail = q1 - 1.5 * iqr\n",
    "    upper_tail = q3 + 1.5 * iqr\n",
    "    return df[column].apply(lambda x: np.nan if x < lower_tail or x > upper_tail else x)\n",
    "\n",
    "columns_to_check = ['PM2.5', 'PM10', 'PM1', 'CO', 'CO2']\n",
    "\n",
    "# Copy original dataframes\n",
    "df_cleaned_Out_RH = df_RH.copy()\n",
    "df_cleaned_Out_ZI = df_ZI.copy()\n",
    "\n",
    "# Apply the function to each column in df_cleaned_Out_RH\n",
    "for col in columns_to_check:\n",
    "    df_cleaned_Out_RH[col] = remove_outliers(df_cleaned_Out_RH, col)\n",
    "\n",
    "# Apply the function to each column in df_cleaned_Out_ZI\n",
    "for col in columns_to_check:\n",
    "    df_cleaned_Out_ZI[col] = remove_outliers(df_cleaned_Out_ZI, col)\n",
    "\n",
    "print(df_cleaned_Out_RH)\n",
    "print(df_cleaned_Out_ZI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "rmse = lambda y, yhat: np.sqrt(mean_squared_error(y, yhat))\n",
    "def optimize_k(data, target):\n",
    "    errors = []\n",
    "    for k in range(1, 40, 1):\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed = imputer.fit_transform(data)\n",
    "        df_imputed = pd.DataFrame(imputed, columns=data.columns)\n",
    "        \n",
    "        X = df_imputed.drop(target, axis=1)\n",
    "        y = df_imputed[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        error = rmse(y_test, preds)\n",
    "        errors.append({'K': k, 'RMSE': error})\n",
    "        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d199d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_errors = optimize_k(data=df_cleaned_Out_ZI, target='PM2.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "imputed = imputer.fit_transform(df_cleaned_Out_ZI)\n",
    "df_cleaned_ZI = pd.DataFrame(imputed, columns=df_cleaned_Out_ZI.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=8)\n",
    "imputed = imputer.fit_transform(df_cleaned_Out_RH)\n",
    "df_cleaned_RH = pd.DataFrame(imputed, columns=df_cleaned_Out_RH.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRH=df_cleaned_RH\n",
    "dataZI=df_cleaned_ZI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795042f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataZI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36044271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataRH[\"date\"]=pd.to_datetime(dataRH[['Year', 'Month', 'Day', 'Hour']])      \n",
    "dataZI[\"date\"]=pd.to_datetime(dataZI[['Year', 'Month', 'Day', 'Hour']])  \n",
    "\n",
    "dataRH = dataRH.set_index(pd.DatetimeIndex(dataRH['date']))\n",
    "dataZI = dataZI.set_index(pd.DatetimeIndex(dataZI['date']))\n",
    "\n",
    "dataRH = dataRH.drop(['date'], axis=1)\n",
    "dataZI = dataZI.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataZI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b253adc",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f7b6b",
   "metadata": {},
   "source": [
    "## LGBMRegressor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d121631",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataZI.drop(columns = ['PM2.5','PM10'])\n",
    "y = dataZI['PM2.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lightgbm import LGBMRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Define the search space\n",
    "search_spaces = {\n",
    "    'num_leaves': Integer(10, 50),\n",
    "    'min_child_samples': Integer(1, 100),\n",
    "    'subsample': Real(1e-8, 1.0, 'log-uniform'),\n",
    "    'colsample_bytree': Real(1e-8, 1.0, 'log-uniform'),\n",
    "    'reg_alpha': Real(1e-8, 1.0, 'log-uniform'),\n",
    "    'reg_lambda': Real(1e-8, 1.0, 'log-uniform'),\n",
    "    'learning_rate': Real(1e-8, 1.0, 'log-uniform'),\n",
    "    'max_depth': Integer(-1, 50),\n",
    "    'n_estimators': Integer(10, 2000)\n",
    "}\n",
    "\n",
    "# Initialize BayesSearchCV\n",
    "lgbm_opt = BayesSearchCV(\n",
    "    estimator=LGBMRegressor(),\n",
    "    search_spaces=search_spaces,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lgbm_opt.fit(X, y)\n",
    "\n",
    "# Get the best estimator\n",
    "best_gbm = lgbm_opt.best_estimator_\n",
    "\n",
    "# Predict using the best estimator\n",
    "lgbm_tuned_fcst = best_gbm.predict(X)\n",
    "\n",
    "# Calculate and print R^2 score\n",
    "r2 = r2_score(y, lgbm_tuned_fcst)\n",
    "print(f\"R^2 score: {r2}\")\n",
    "\n",
    "# Feature importance from the best estimator\n",
    "fea_imp_ = pd.DataFrame({'cols': X.columns, 'fea_imp': best_gbm.feature_importances_})\n",
    "fea_imp_sorted = fea_imp_.loc[fea_imp_.fea_imp > 0].sort_values(by='fea_imp', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"fea_imp\", y=\"cols\", data=fea_imp_sorted)\n",
    "plt.title('LightGBM Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('lgbm_importances-01.png')\n",
    "\n",
    "# Create a DataFrame of sorted feature importances for further use if needed\n",
    "feature_importances_ = pd.DataFrame(sorted(zip(best_gbm.feature_importances_, X.columns)), columns=['Value', 'Feature'])\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "\n",
    "# Create the RFE model and select 7 attributes\n",
    "rfe = RFE(estimator=best_gbm, n_features_to_select=7)\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# Summarize the selection of the attributes\n",
    "print(\"Selected features:\", rfe.support_)\n",
    "\n",
    "# Summarize the ranking of the attributes\n",
    "fea_rank_ = pd.DataFrame({'cols': X.columns, 'fea_rank': rfe.ranking_})\n",
    "fea_rank_sorted = fea_rank_.sort_values(by='fea_rank', ascending=True)\n",
    "\n",
    "# Print the ranking of features\n",
    "print(fea_rank_sorted)\n",
    "\n",
    "# Optional: Print only the selected features\n",
    "selected_features = fea_rank_sorted[fea_rank_sorted['fea_rank'] == 1]\n",
    "print(\"Selected features ranked as 1:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFE_data=dataZI[['RH','RT','windgust','PM2.5' ,'winddir','CO2','humidity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342a7a3",
   "metadata": {},
   "source": [
    "# mRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataZI.drop(columns = ['PM2.5','PM10'])\n",
    "y = dataZI['PM2.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed_df = pd.DataFrame(X, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mrmr\n",
    "from mrmr import mrmr_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF = mrmr_regression(X_imputed_df, y, K = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f67c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mr_data=dataZI[['PM2.5', 'windgust', 'temp', 'feelslike', 'sealevelpressure', 'humidity', 'CO2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee763c6",
   "metadata": {},
   "source": [
    "# Modelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, train_size=0.5, n_in=1, n_out=1, target_column='target', dropnan=True, scale_X=True):\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # Make sure the target column is the last column in the dataframe\n",
    "    df['PM2'] = df[target_column] # Make a copy of the target column\n",
    "    df = df.drop(columns=[target_column]) # Drop the original target column\n",
    "\n",
    "    target_location = df.shape[1] - 1 # column index number of target\n",
    "\n",
    "    # ...X\n",
    "    #X = df.iloc[:, :target_location]\n",
    "    X = df.iloc[:,:]\n",
    "\n",
    "    # ...y\n",
    "    y = df.iloc[:, [target_location]]\n",
    "\n",
    "    # Scale the features\n",
    "    if scale_X:\n",
    "        #col_names=['target']\n",
    "        #features = X[col_names]\n",
    "        features = X[X.columns]\n",
    "        scalerX = MinMaxScaler().fit(features.values)\n",
    "        features = scalerX.transform(features.values)\n",
    "\n",
    "        #X['target'] = features\n",
    "        X[X.columns] = features\n",
    "\n",
    "    #n_vars_x = X.shape[1]\n",
    "    x_vars_labels = X.columns\n",
    "    y_vars_labels = y.columns\n",
    "\n",
    "    x_cols, x_names = list(), list()\n",
    "    y_cols, y_names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        x_cols.append(X.shift(i))\n",
    "        x_names += [('%s(t-%d)' % (j, i)) for j in x_vars_labels]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        y_cols.append(y.shift(-i))\n",
    "        if i == 0:\n",
    "            y_names += [('%s(t)' % (j)) for j in y_vars_labels]\n",
    "        else:\n",
    "            y_names += [('%s(t-%d)' % (j, i)) for j in y_vars_labels]\n",
    "\n",
    "    # put it all together\n",
    "    x_agg = pd.concat(x_cols, axis=1)\n",
    "    x_agg.columns = x_names\n",
    "\n",
    "    y_agg = pd.concat(y_cols, axis=1)\n",
    "    y_agg.columns = y_names\n",
    "\n",
    "    agg=pd.concat([x_agg,y_agg], axis=1)\n",
    "    agg.columns = x_names + y_names\n",
    "    #print(agg)\n",
    "\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        x_agg.dropna(inplace=True)\n",
    "        y_agg.dropna(inplace=True)\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    \"\"\"\n",
    "    diff = y_agg.shape[0] - x_agg.shape[0]\n",
    "    idx = [i for i in range(0, diff)]\n",
    "    y_agg = y_agg.drop(df.index[idx])\"\"\"\n",
    "\n",
    "    nf = X.shape[1]\n",
    "    xx = agg.iloc[:,:n_in*nf]\n",
    "    yy = agg.iloc[:,-n_out:]\n",
    "\n",
    "    split_index = int(xx.shape[0]*train_size) # the index at which to split df into train and test\n",
    "\n",
    "    # ...train\n",
    "    X_train = xx.iloc[:split_index, :]\n",
    "    y_train = yy.iloc[:split_index, ]\n",
    "\n",
    "    # ...test\n",
    "    X_test = xx.iloc[split_index:, :] # original is split_index:-1\n",
    "    y_test = yy.iloc[split_index:, ] # original is split_index:-1\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scale_X\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_explained_variance(y_true, y_pred):\n",
    "    return np.var(y_true - y_pred) / np.var(y_true)\n",
    "\n",
    "def calculate_medae(y_true, y_pred):\n",
    "    return np.median(np.abs(y_true - y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff815910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor \n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, explained_variance_score, median_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413758ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=dataZI\n",
    "dataset=RFE_data\n",
    "#dataset=Mr_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c783782",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, scale_X = series_to_supervised(dataset, train_size=0.7, n_in=24, n_out=1, target_column='PM2.5', dropnan=True, scale_X=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d740214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "\n",
    "# Fit and evaluate models\n",
    "models = {\n",
    "    \"LinearRegression\": MultiOutputRegressor(LinearRegression()),\n",
    "    \"RandomForestRegressor\": MultiOutputRegressor(RandomForestRegressor(max_depth=26, min_samples_leaf=16, min_samples_split=43, n_estimators=400)),\n",
    "    \"GradientBoostingRegressor\": MultiOutputRegressor(GradientBoostingRegressor(learning_rate=0.08357797442759243, n_estimators=400, max_depth=26, min_samples_leaf=16, min_samples_split=43)),\n",
    "    \"SVR\": MultiOutputRegressor(SVR(C=7.2184767589092855, epsilon=0.0004140476127681186, gamma=0.029462473319069343)),\n",
    "    \"LGBMRegressor\": MultiOutputRegressor(LGBMRegressor(reg_lambda=12.203617911779522, alpha=11.974507437260828, colsample_bytree=1, colsample_bynode=0.8, learning_rate=0.02, n_estimators=800, max_depth=4, min_child_weight=72, subsample=0.5382348429040217)),\n",
    "    \"XGBRegressor\": MultiOutputRegressor(XGBRegressor(reg_lambda=0.0014159666728948113, alpha=0.035222713638458546, colsample_bytree=0.902004429582736, colsample_bynode=0.7786908529650358, learning_rate=0.017244790780114542, n_estimators=581, max_depth=5, min_child_weight=121, subsample=0.7985209953151778))\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'R2 Test': round(r2_score(y_test, test_pred), 4),\n",
    "        'RMSE Test': round(np.sqrt(mean_squared_error(y_test, test_pred)), 4),\n",
    "        'MAE Test': round(mean_absolute_error(y_test, test_pred), 4),\n",
    "        #'SMAPE Test': round(smape(y_test, test_pred), 4),\n",
    "        'MSE Test': round(calculate_mse(y_test, test_pred), 4),\n",
    "        #'MAPE Test': round(calculate_mape(y_test, test_pred), 4),\n",
    "        'Explained Variance Test': round(calculate_explained_variance(y_test, test_pred), 4),\n",
    "        'MedAE Test': round(calculate_medae(y_test, test_pred), 4)\n",
    "    }\n",
    "    \n",
    "    results.append(metrics)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame as a formatted table\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model on train data \n",
    "LGBM=MultiOutputRegressor(LGBMRegressor( reg_lambda= 12.203617911779522, alpha = 11.974507437260828, colsample_bytree= 1,\n",
    "colsample_bynode= 0.8,learning_rate= 0.02, n_estimators= 800, max_depth= 4, min_child_weight= 72, subsample= 0.5382348429040217))\n",
    "\n",
    "LGBM.fit(X_train,y_train)\n",
    "#predict on train \n",
    "train_lgbm = LGBM.predict(X_train)\n",
    "test_lgbm = LGBM.predict(X_test)\n",
    "print('-'*50)\n",
    "#fit the model on train data \n",
    "XGB=XGBRegressor( reg_lambda= 0.0014159666728948113, alpha = 0.035222713638458546, colsample_bytree= 0.902004429582736,\n",
    " colsample_bynode= 0.7786908529650358,learning_rate= 0.017244790780114542, n_estimators= 581, max_depth= 5, min_child_weight= 121, subsample= 0.7985209953151778)\n",
    "XGB.fit(X_train,y_train)\n",
    "#predict on train \n",
    "train_xgb = XGB.predict(X_train)\n",
    "test_xgb = XGB.predict(X_test)\n",
    "print('-'*50)\n",
    "\n",
    "#fit the model on train data \n",
    "SV=SVR(C=7.2184767589092855, epsilon=0.0004140476127681186,\n",
    "    gamma=0.029462473319069343)\n",
    "SV.fit(X_train,y_train)\n",
    "#predict on train \n",
    "train_SVR = SV.predict(X_train)\n",
    "#predict on test\n",
    "test_SVR = SV.predict(X_test)\n",
    "#accuracy on test\n",
    "print('-'*50)\n",
    "\n",
    "#fit the model on train data \n",
    "GB=GradientBoostingRegressor(learning_rate=0.08357797442759243, n_estimators=400,max_depth=26, min_samples_leaf=16, min_samples_split=43)\n",
    "GB.fit(X_train,y_train)\n",
    "#predict on train \n",
    "train_GB = GB.predict(X_train)\n",
    "#predict on test\n",
    "test_GB = GB.predict(X_test)\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "#fit the model on train data \n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "#predict on train \n",
    "train_lin = lin_reg.predict(X_train)\n",
    "#predict on test\n",
    "test_lin = lin_reg.predict(X_test)\n",
    "print('-'*50)\n",
    "\n",
    "#fit the model on train data \n",
    "RF=RandomForestRegressor(max_depth=26, min_samples_leaf=16, min_samples_split=43,n_estimators=400)\n",
    "#RF=MultiOutputRegressor(RandomForestRegressor())\n",
    "RF.fit(X_train,y_train)\n",
    "#predict on train \n",
    "train_rf = RF.predict(X_train)\n",
    "#predict on test\n",
    "test_rf = RF.predict(X_test)\n",
    "\n",
    "print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584aa3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Create fig size for plotting\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Plot the true values \n",
    "ax.plot(y_test.index, y_test, color='black', label='Observed', marker='o', markerfacecolor='black', markersize=8, linestyle='-', linewidth=2)\n",
    "\n",
    "# Plot the predictions\n",
    "ax.plot(y_test.index, test_lgbm, color='#1f77b4', label='LightGBM', marker='s', markerfacecolor='#1f77b4', markersize=8, linestyle='--', linewidth=2)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Comparison of Observed and Predicted PM2.5 Concentrations', fontweight=\"bold\", fontsize=24, pad=20)\n",
    "ax.set_xlabel('Date and Time', fontweight=\"bold\", fontsize=20, labelpad=15)\n",
    "ax.set_ylabel('PM2.5 Concentration (µg/m³)', fontweight=\"bold\", fontsize=20, labelpad=15)\n",
    "\n",
    "# Customize the ticks\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "# Set legend\n",
    "ax.legend(loc=\"upper right\", fontsize=16, title=\"Legend\", title_fontsize='16')\n",
    "\n",
    "# Format the x-axis with date labels\n",
    "xfmt = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "ax.xaxis.set_major_formatter(xfmt)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add grid lines for better readability\n",
    "ax.grid(axis='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "# Save the figure with high resolution\n",
    "plt.savefig(\"fit_curve_PM25.png\", dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Create fig size for plotting\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Plot the true values \n",
    "ax.plot(y_test[:24].index, y_test[:24], color='black', label='True', marker='o', markerfacecolor='black', markersize=8, linestyle='-', linewidth=2)\n",
    "\n",
    "# Plot the predictions\n",
    "ax.plot(y_test[:24].index, test_lgbm[:24], color='#1f77b4', label='LightGBM', marker='s', markerfacecolor='#1f77b4', markersize=8, linestyle='--', linewidth=2)\n",
    "ax.plot(y_test[:24].index, test_GB[:24], color='#ff7f0e', label='GBRT', marker='D', markerfacecolor='#ff7f0e', markersize=8, linestyle='--', linewidth=2)\n",
    "ax.plot(y_test[:24].index, test_xgb[:24], color='#2ca02c', label='XGBoost', marker='^', markerfacecolor='#2ca02c', markersize=8, linestyle='--', linewidth=2)\n",
    "ax.plot(y_test[:24].index, test_rf[:24], color='#d62728', label='RF', marker='v', markerfacecolor='#d62728', markersize=8, linestyle='--', linewidth=2)\n",
    "ax.plot(y_test[:24].index, test_SVR[:24], color='#9467bd', label='SVR', marker='p', markerfacecolor='#9467bd', markersize=8, linestyle='--', linewidth=2)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('LightGBM Feature Selection with 24h Lags', fontweight=\"bold\", fontsize=24, pad=20)\n",
    "ax.set_xlabel('Hour', fontweight=\"bold\", fontsize=20, labelpad=15)\n",
    "ax.set_ylabel('PM2.5 Concentration (µg/m³)', fontweight=\"bold\", fontsize=20, labelpad=15)\n",
    "\n",
    "# Customize the ticks\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "# Set legend\n",
    "ax.legend(loc=\"upper right\", fontsize=16, title=\"Models\", title_fontsize='16')\n",
    "\n",
    "# Format the x-axis with date labels\n",
    "xfmt = mdates.DateFormatter('%H:%M')\n",
    "ax.xaxis.set_major_formatter(xfmt)\n",
    "\n",
    "# Add grid lines for better readability\n",
    "ax.grid(axis='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "# Save the figure with high resolution\n",
    "plt.savefig(\"observed_predicted_PM25.png\", dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87603507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have a fitted MultiOutputRegressor model (LGBM)\n",
    "# and your X_test is the test set\n",
    "\n",
    "# Create a TreeExplainer for each output\n",
    "shap_values_list = []\n",
    "for estimator in LGBM.estimators_:\n",
    "    explainer = shap.TreeExplainer(estimator)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap_values_list.append(shap_values)\n",
    "\n",
    "# Plot SHAP values for the first output as an example\n",
    "shap_values = shap_values_list[0]\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Adjust the height by changing the figsize parameter (width, height)\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "shap.summary_plot(\n",
    "    shap_values, X_test, feature_names=X_test.columns, plot_type=\"bar\", show=False\n",
    ")\n",
    "\n",
    "# Customize the plot to match PLOS ONE style\n",
    "plt.title(\"Feature Importance based on SHAP Values\", fontsize=20, weight='bold', pad=20)\n",
    "plt.xlabel('Mean |SHAP Value| (Impact on Model Output)', fontsize=16, weight='bold', labelpad=15)\n",
    "plt.ylabel('Features', fontsize=16, weight='bold', labelpad=15)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure with high resolution\n",
    "plt.savefig(\"feature_importance_plot.png\", dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have a fitted MultiOutputRegressor model (LGBM)\n",
    "# and your X_test is the test set\n",
    "\n",
    "# Create a TreeExplainer for each output\n",
    "shap_values_list = []\n",
    "for estimator in LGBM.estimators_:\n",
    "    explainer = shap.TreeExplainer(estimator)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap_values_list.append(shap_values)\n",
    "\n",
    "# Plot SHAP values for the first output as an example\n",
    "shap_values = shap_values_list[0]\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Adjust the height by changing the figsize parameter (width, height)\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"dot\", show=False)\n",
    "\n",
    "# Customize the plot to match PLOS ONE style\n",
    "plt.title(\"SHAP Summary Plot\", fontsize=20, weight='bold', pad=20)\n",
    "plt.xlabel('SHAP Value (impact on model output)', fontsize=16, weight='bold', labelpad=15)\n",
    "plt.ylabel('Features', fontsize=16, weight='bold', labelpad=15)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure with high resolution\n",
    "plt.savefig(\"shap_values.png\", dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9733387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
